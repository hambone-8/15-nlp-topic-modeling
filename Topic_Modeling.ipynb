{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Topic Modeling - What ARE they talkingabout?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "- Demonstrate an understanding of clustering/topic approaches with NLP\n",
    "- Understand approaches to persisting the insights from unsupervised approaches\n",
    "- Discuss keyword generation and classification using machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:39.379641Z",
     "start_time": "2023-01-17T13:48:39.375182Z"
    }
   },
   "outputs": [],
   "source": [
    "# from IPython.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "# display(HTML(\"<style>.output_result { max-width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:11.190157Z",
     "start_time": "2023-01-17T13:48:11.179715Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from gensim import matutils, corpora\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import MmCorpus, dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#import pyLDAvis.gensim_models\n",
    "\n",
    "pd.options.display.float_format = '{:,.8f}'.format\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:11.687133Z",
     "start_time": "2023-01-17T13:48:11.679274Z"
    }
   },
   "outputs": [],
   "source": [
    " # Due to backward incompatibility, if spacy 2.x is used, the codes here will not work as intended\n",
    "assert spacy.__version__ >= \"3.0.0\", \"Ensure spacy version is 3.x.x\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "Topic modeling is an interesting problem in NLP applications where we want to get an idea of what subjects (topics) we have in our dataset. A topic is nothing more than a collection of words that describe the overall theme. For example, in case of news articles, we might think of topics as politics, sports etc. \n",
    "\n",
    "Topic modeling won’t directly give you names of the topics but rather a set of most probable words that might describe a topic. It is up to us to determine what topic the set of words might refer to. \n",
    "\n",
    "In the below we see a recipe for various topics - **What do you think they represent?**\n",
    "\n",
    "![TopicModeling](assets/TopicModeling.png)\n",
    "\n",
    "#### All topic models are based on the same basic assumption:\n",
    "- each document consists of a mixture of topics, and\n",
    "- each topic consists of a collection of words.\n",
    "\n",
    "\n",
    "In other words, topic models are built around the idea that the semantics of our document are actually being governed by some hidden, or “latent,” variables that we are not observing. As a result, the goal of topic modeling is to uncover these latent variables — topics — that shape the meaning of our document and corpus. \n",
    "\n",
    "For example, the word bank when used together with mortgage, loans, and rates probably means a financial institution. However, the word bank when used together with lures, casting, and fish probably means a stream or river bank.\n",
    "\n",
    "If every word natively assigned itself to a topic - that would not be an issue\n",
    "\n",
    "![MachineLearning](assets/word2concept.png)\n",
    "\n",
    "Unfortunately - with context it tends to be more confusing\n",
    "\n",
    "![MachineLearning](assets/concept2words.png)\n",
    "\n",
    "The fundamental difficulty arises when we compare words to find relevant documents, because what we really want to do is compare the meanings or concepts behind the words. To do that we use topic models to build topic 'recipes'. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Recipes\n",
    "\n",
    "When the model builds out topics - it creates a frequency of different unique words that define the topic. However, the model does not convey the \"topic\" of conversation. It's up to the Data Scientist to piece it together with context clues. There are a few ways to do this:\n",
    "\n",
    "1. Look at the words and frequencies to see if they tell a story\n",
    "2. Investigate the records by calling back multiple records per topics or the most dominant topics per record \n",
    "\n",
    "Often time you may need SME to assist!\n",
    "\n",
    "\n",
    "For example - say we have three topics - sports, business and science. We ask our model to find 3 topics (a number we have to tune) and it returns the below recipes where each word and topic pair includes the probability **P(word|topic)**\n",
    "\n",
    "- Topic 1: [football: 0.3, basketball: 0.2, baseball: 0.2, touchdown: 0.02 ... genetics: 0.0001] \n",
    "- Topic 2: [genetics: 0.2, drug: 0.2, ... baseball: 0.0001]\n",
    "- Topic 3: [stocks: 0.1, ipo: 0.08,  ... baseball: 0.0001]\n",
    "\n",
    "**How would you label these topics?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With that in mind we'll investigate a few methods of topic modeling \n",
    "\n",
    "### Today's Topic Modeling Approaches\n",
    "- Latent Semantic Analysis\n",
    "- Latent Direchlet Analysis\n",
    "\n",
    "### Others\n",
    "- nonnegative matrix factorization(NMF)\n",
    "- Probabilistic Latent Semantic Analysis\n",
    "- lda2vec\n",
    "\n",
    "[Read up on a few of these here](https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis\n",
    "\n",
    "Latent Semantic Analysis (LSA), also known as Latent Semantic Indexing (LSI) literally means analyzing documents to find the underlying meaning or concepts of those documents. If each word only meant one concept, and each concept was only described by one word, then LSA would be easy since there is a simple mapping from words to concepts.\n",
    "\n",
    "Unfortunately, this problem is difficult because English has different words that mean the same thing (synonyms), words with multiple meanings, and all sorts of ambiguities that obscure the concepts to the point where even people can have a hard time understanding.\n",
    "\n",
    "\n",
    "\n",
    "#### Step 1 - Generating a Document Term Matrix\n",
    "\n",
    "Given m document with n words in our vocabulary we can construct a large matrix (m X n). Each row represents a document (individual record) and each column represents every unique word from the corpus. While the method could count the number of words that would overlook the significance so typically LSA models replace raw counts with tf-idf scores. \n",
    "\n",
    "#### Step 2 - Dimensionality reduction\n",
    "\n",
    "With every unique word captured as a column the resulting matrix is very likely spare and noisy. A traditional approach is using singular value decomposition. This linear algebra technique factorizes any matrix into a product of 3 seperate matrices: \n",
    "\n",
    "\n",
    "##### Step 3 - Uncover Topics\n",
    "\n",
    "With these document vectors and term vectors, we can now easily apply measures such as cosine similarity to evaluate the similarity of different documents, words or passages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:11.860742Z",
     "start_time": "2023-01-17T13:48:11.854369Z"
    }
   },
   "outputs": [],
   "source": [
    "#For simplicities sake - let's start with a small dataset\n",
    "\n",
    "documents = [\"Rover is a good dog\", \n",
    "             \"Cats are lazy\", \n",
    "             \"dogs are a mans best friend\" , \n",
    "             \"There is the cat in the hat\", \n",
    "             \"Cats are easy to care for\", \n",
    "             \"I only want black cats and dogs\", \n",
    "             \"Have you seen my dog?\"] \n",
    "\n",
    "dog_cat=pd.DataFrame(documents, columns=['documents'])\n",
    "print (dog_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:11.914641Z",
     "start_time": "2023-01-17T13:48:11.901647Z"
    }
   },
   "outputs": [],
   "source": [
    "dog_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with some preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about regex, visit https://regexr.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:12.355750Z",
     "start_time": "2023-01-17T13:48:12.347152Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove special characters\n",
    "dog_cat['clean_documents'] = dog_cat['documents'].str.replace(\"[^a-zA-Z#]\", \" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:12.554860Z",
     "start_time": "2023-01-17T13:48:12.537171Z"
    }
   },
   "outputs": [],
   "source": [
    "dog_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:12.701636Z",
     "start_time": "2023-01-17T13:48:12.686040Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove words have letters less than 3\n",
    "dog_cat['clean_documents'] = dog_cat['clean_documents'].fillna('').apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:12.850755Z",
     "start_time": "2023-01-17T13:48:12.823534Z"
    }
   },
   "outputs": [],
   "source": [
    "#lowercase all characters\n",
    "dog_cat['clean_documents'] = dog_cat['clean_documents'].fillna('').apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:13.000556Z",
     "start_time": "2023-01-17T13:48:12.974256Z"
    }
   },
   "outputs": [],
   "source": [
    "dog_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:13.322310Z",
     "start_time": "2023-01-17T13:48:13.290967Z"
    }
   },
   "outputs": [],
   "source": [
    "#Build the TF-IDF DTM\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             use_idf=True, \n",
    "                             smooth_idf=True)\n",
    "\n",
    "# SVD to reduce dimensionality: \n",
    "svd_model = TruncatedSVD(n_components=2,         \n",
    "                         algorithm='randomized',\n",
    "                         n_iter=20)\n",
    "\n",
    "# Using a pipeline for fun to combine tf-idf + SVD fitting against corpus\n",
    "svd_transformer = Pipeline([('tfidf', vectorizer), \n",
    "                            ('svd', svd_model)])\n",
    "svd_matrix = svd_transformer.fit_transform(dog_cat['clean_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:13.483976Z",
     "start_time": "2023-01-17T13:48:13.473419Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's capture the output of our topic model\n",
    "topic_encoded_df = pd.DataFrame(svd_matrix, columns = [\"topic_1\", \"topic_2\"])\n",
    "topic_encoded_df[\"documents\"] = dog_cat['clean_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:13.984276Z",
     "start_time": "2023-01-17T13:48:13.970564Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using the below - which topic is for cats and which one is for dogs?\n",
    "topic_encoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Repeat the above using the below documents\n",
    "\n",
    "\n",
    "Items for consideration\n",
    "- How many topics are you looking to find?\n",
    "- How can you best label the topics?\n",
    "- Will this model work well on a new record? When and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:14.030122Z",
     "start_time": "2023-01-17T13:48:14.027042Z"
    }
   },
   "outputs": [],
   "source": [
    "titles =[ \"The Neatest Little Guide to Stock Market Investing\", \n",
    "    \"Investing For Dummies, 4th Edition\", \n",
    "    \"The Little Book of Common Sense Investing: The Only Way to Guarantee Your Fair Share of Stock Market Returns\", \n",
    "    \"The Little Book of Value Investing\", \n",
    "    \"Value Investing: From Graham to Buffett and Beyond\", \n",
    "    \"Rich Dad's Guide to Investing: What the Rich Invest in, That the Poor and the Middle Class Do Not!\", \n",
    "    \"Investing in Real Estate, 5th Edition\", \n",
    "    \"Stock Investing For Dummies\", \n",
    "    \"Rich Dad's Advisors: The ABC's of Real Estate Investing: The Secrets of Finding Hidden Profits Most Investors Miss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:14.076180Z",
     "start_time": "2023-01-17T13:48:14.063720Z"
    }
   },
   "outputs": [],
   "source": [
    "# Workspace\n",
    "df = pd.DataFrame(titles, columns=['titles'])\n",
    "print (df.shape)\n",
    "#df.head()\n",
    "\n",
    "df['clean_documents'] = df['titles'].str.replace(\"[^a-zA-Z#]\", \" \", regex=True)\n",
    "df['clean_documents'] = df['clean_documents'].fillna('').apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "df['clean_documents'] = df['clean_documents'].fillna('').apply(lambda x: x.lower())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:14.121497Z",
     "start_time": "2023-01-17T13:48:14.105237Z"
    }
   },
   "outputs": [],
   "source": [
    "#Build the TF-IDF DTM\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             use_idf=True, \n",
    "                             smooth_idf=True)\n",
    "\n",
    "# SVD to reduce dimensionality: \n",
    "svd_model = TruncatedSVD(n_components=2,         \n",
    "                         algorithm='randomized',\n",
    "                         n_iter=20)\n",
    "\n",
    "# Using a pipeline for fun to combine tf-idf + SVD fitting against corpus\n",
    "svd_transformer = Pipeline([('tfidf', vectorizer), \n",
    "                            ('svd', svd_model)])\n",
    "svd_matrix = svd_transformer.fit_transform(df['clean_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:14.151396Z",
     "start_time": "2023-01-17T13:48:14.139166Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's capture the output of our topic model\n",
    "topic_encoded_df = pd.DataFrame(svd_matrix, columns = [\"topic_1\", \"topic_2\"])\n",
    "topic_encoded_df[\"documents\"] = df['clean_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:14.182953Z",
     "start_time": "2023-01-17T13:48:14.169173Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_encoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA Overview:\n",
    "\n",
    "Pros:\n",
    "- Quick and Efficient method    \n",
    "    \n",
    "Cons:\n",
    "- lack of interpretable topics means we have to assign context ourselves \n",
    "- the components may be arbitrarily positive/negative\n",
    "- need for really large set of documents and vocabulary to get accurate results\n",
    "- less efficient representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Latent Dirichlet allocation (LDA) is a topic model that generates topics based on word frequency from a set of documents. LDA is particularly useful for finding reasonably accurate mixtures of topics within a given document set. We can think of dirichlet as a “distribution over distributions.” In essence, it answers the question: “given this type of distribution, what are some actual probability distributions I am likely to see?”\n",
    "\n",
    "![LDA](assets/Schematic-of-LDA.png)\n",
    "\n",
    "\n",
    "Essentially Latent Dirichlet Allocation is a model that assumes this is the way text is generated and then attempts to learn two things:\n",
    "\n",
    "1. The word distribution of each topic\n",
    "\n",
    "2. The topic distribution of each document.\n",
    "\n",
    "In the below graph you'll notice how the dirichlet distribution allows multiple words to be associated with various topics, and multiple documents to be assigned as well. It's an expanding infered context that allows both diminsions to play a role in defining the creation of topis.\n",
    "\n",
    "![ldamatching](assets/ldamatching.jpeg)\n",
    "\n",
    "\n",
    "LDA is easily the most popular topic modeling technique out there. It's not a silver bullet but works well in a wide range of applications and is easy to use. The basic steps are\n",
    "\n",
    "1. Loading & clean data\n",
    "2. Preparing data for LDA analysis\n",
    "3. LDA model training\n",
    "4. Analyzing LDA model results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for this journey \n",
    "\n",
    "For the LDA model we're going to kick it up a notch. We will be using 1,000 papers from the Neural Information Processing Systems (NeurIPS) conferences from 1987 to 2016. As the file is ~400 mb it has been reduced to papers listing an abstract\n",
    "\n",
    "\n",
    "https://www.kaggle.com/benhamner/nips-papers?select=database.sqlite\n",
    "\n",
    "\n",
    "### Step 1: Load and Clean data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:14.292030Z",
     "start_time": "2023-01-17T13:48:14.268648Z"
    }
   },
   "outputs": [],
   "source": [
    "papers=pd.read_csv('data/NIPS_Papers.csv')\n",
    "print (papers.shape)\n",
    "papers.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:14.307016Z",
     "start_time": "2023-01-17T13:48:14.300798Z"
    }
   },
   "outputs": [],
   "source": [
    "papers.iloc[0].abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:14.368837Z",
     "start_time": "2023-01-17T13:48:14.334457Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "papers['abstract_processed'] = papers['abstract'].map(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "papers['abstract_processed'] = papers['abstract_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows of papers\n",
    "papers['abstract_processed'][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:15.026888Z",
     "start_time": "2023-01-17T13:48:15.016565Z"
    }
   },
   "outputs": [],
   "source": [
    "papers.iloc[0].abstract # Before processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:15.419758Z",
     "start_time": "2023-01-17T13:48:15.405112Z"
    }
   },
   "outputs": [],
   "source": [
    "papers.iloc[0].abstract_processed # After processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Preparing data for LDA analysis\n",
    "\n",
    "We are going to stand this up a bit from how we've lemmatized before. With the power of spacy we can lemmatize much faster based on the POS tagging. The context of nouns, verbs, adjectives and adverbs will be lemmatized against the associated sPacy lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:23.095008Z",
     "start_time": "2023-01-17T13:48:16.074789Z"
    }
   },
   "outputs": [],
   "source": [
    "#Lemmatize the data to reduce the feature space\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "papers['abstract_lemma'] = papers['abstract_processed'].map(lambda x: [token.lemma_ for token in nlp(x) if token.lemma_ != '-PRON-' and token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV'}])\n",
    "\n",
    "# Final cleaning\n",
    "papers['abstract_processed_lemma'] = papers['abstract_lemma'].map(lambda x: [t for t in x if len(t) > 1])\n",
    "\n",
    "# Example\n",
    "print(papers['abstract_processed_lemma'].iloc[0][:25], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:23.096006Z",
     "start_time": "2023-01-17T13:48:23.096006Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's remove stopwords and see the difference\n",
    "stop_en = stopwords.words('english')\n",
    "papers['abstract_processed_lemma'] = papers['abstract_processed_lemma'].map(lambda x: [t for t in x if t not in stop_en]) \n",
    "print(papers['abstract_processed_lemma'].iloc[0][:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:46:13.589046Z",
     "start_time": "2023-01-17T13:46:13.518452Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a corpus from a list of texts\n",
    "texts = papers.sample(n=500, random_state=43)['abstract_processed_lemma'].values\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:46:13.604311Z",
     "start_time": "2023-01-17T13:46:13.592081Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's take a sneak peak inside dictionary\n",
    "print(f'No of words stored in dictionary: {len(dictionary.items())}') \n",
    "print ('10 most commonly used words in dictionary:')\n",
    "print (dictionary.most_common()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: build our LDA Model\n",
    "\n",
    "More parameters [can be found here](https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    "\n",
    "`corpus`– Stream of document vectors or sparse matrix of shape (num_terms, num_documents). If not given, the model is left untrained (presumably because you want to call update() manually).\n",
    "\n",
    "`num_topics`  – The number of requested latent topics to be extracted from the training corpus.\n",
    "\n",
    "`id2word` – Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "\n",
    "`minimum_probability` – Topics with a probability lower than this threshold will be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:46:14.256131Z",
     "start_time": "2023-01-17T13:46:13.604311Z"
    }
   },
   "outputs": [],
   "source": [
    "n_topics=9\n",
    "\n",
    "my_lda = LdaModel(corpus, num_topics=n_topics, id2word=dictionary, random_state=42, minimum_probability=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first topics\n",
    "\n",
    "With those few lines of code we've explored and built an NLP version of clustering to create our topics. The next step is to explore them.\n",
    "\n",
    "We can limit what is returned by specifying the number of topics we're interested in and the number of words to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:46:14.270030Z",
     "start_time": "2023-01-17T13:46:14.256131Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can limit what is returned by specifying the number of topics we're interested in and the number of works to return\n",
    "num_topics = 10\n",
    "num_words = 5\n",
    "for ti, topic in enumerate(my_lda.show_topics(num_topics = num_topics, num_words= num_words)):\n",
    "    print(\"Topic: %d\" % (ti))\n",
    "    print (topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:46:25.872858Z",
     "start_time": "2023-01-17T13:46:25.861980Z"
    }
   },
   "outputs": [],
   "source": [
    "# How similar are topics? Remember cosine similarity from above. It's built-in to gensim letting us see how similar topics are to each other\n",
    "\n",
    "matutils.cossim(my_lda.get_topic_terms(2), my_lda.get_topic_terms(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing LDA model results\n",
    "\n",
    "A common method for reviewing a topic model is to investigate its key metrics. For LDA those tend to be\n",
    "\n",
    "#### Perplexity \n",
    " - Perplexity measures how probable some new unseen data is given the model that was learned earlier. That is to say, how well does the model represent or reproduce the statistics of the held-out data. Optimizing for perplexity may not yield human interpretable topics\n",
    "\n",
    "\n",
    "#### Coherence\n",
    " - Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference\n",
    " \n",
    "A good readup on both can be found [here](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0)\n",
    "\n",
    "For C__v coherence scores a rule of thumb is:\n",
    "- .3 is bad\n",
    "\n",
    "\n",
    "- .4 is low\n",
    "\n",
    "\n",
    "- .55 is okay\n",
    "\n",
    "\n",
    "- .65 might be as good as it is going to get\n",
    "\n",
    "\n",
    "- .7 is nice\n",
    "\n",
    "\n",
    "- .8 is unlikely and\n",
    "\n",
    "\n",
    "- .9 is probably wrong\n",
    "\n",
    "My approach to finding the optimal number of topics is to build many LDA models with different values of number of topics (k) and pick the one that gives the highest coherence value.\n",
    "\n",
    "Choosing a ‘k’ that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics. Picking an even higher value can sometimes provide more granular sub-topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:46:31.281937Z",
     "start_time": "2023-01-17T13:46:26.467336Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=my_lda, texts=papers['abstract_processed_lemma'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By our rules of thumb, not a great score so let's see how well we could do given our preprocessing strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.883652Z",
     "start_time": "2023-01-17T13:46:31.283459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Takes few mins to train the models\n",
    "max_topics = 30\n",
    "coh_list = []\n",
    "for n_topics in range(3,max_topics+1):\n",
    "    # Train the model on the corpus\n",
    "    my_lda = LdaModel(corpus, num_topics=n_topics, id2word=dictionary, random_state=42, alpha=0.1)\n",
    "    # Estimate coherence\n",
    "    cm = CoherenceModel(model=my_lda, texts=texts, dictionary=dictionary, coherence='c_v', topn=20)\n",
    "    coherence = cm.get_coherence_per_topic() # get coherence value\n",
    "    coh_list.append(coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.885705Z",
     "start_time": "2023-01-17T13:48:05.885705Z"
    }
   },
   "outputs": [],
   "source": [
    "# Coherence scores:\n",
    "coh_means = np.array([np.mean(l) for l in coh_list])\n",
    "coh_stds = np.array([np.std(l) for l in coh_list])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.xticks(np.arange(3, max_topics+1, 3.0));\n",
    "plt.plot(range(3,max_topics+1), coh_means);\n",
    "plt.fill_between(range(3,max_topics+1), coh_means-coh_stds, coh_means+coh_stds, color='g', alpha=0.05);\n",
    "plt.vlines([8, 9], 0.24, 0.26, color='red', linestyles='dashed',  linewidth=1);\n",
    "plt.hlines([0.253], 3, max_topics, color='black', linestyles='dotted',  linewidth=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks like the best we can do is ~3. In that case we'll want to go back to our data and work on additional stopwords, vectorizing and other pre-processing strategies to make this stronger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations\n",
    "\n",
    "Now that we have a trained model let’s visualize the topics for interpretability. To do so, we’ll use a popular visualization package, pyLDAvis which is designed to help interactively with:\n",
    "1. Better understanding and interpreting individual topics, and\n",
    "2. Better understanding the relationships between the topics.\n",
    "\n",
    "\n",
    "For (1), you can manually select each topic to view its top most frequent and/or “relevant” terms, using different values of the λ parameter. This can help when you’re trying to assign a human interpretable name or “meaning” to each topic.\n",
    "\n",
    "\n",
    "For (2), exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.887701Z",
     "start_time": "2023-01-17T13:48:05.887701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reloading our model with 9 topics\n",
    "n_topics=9\n",
    "n_top_words = 25\n",
    "my_lda = LdaModel(corpus, num_topics=n_topics, id2word=dictionary, random_state=42, minimum_probability=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.888699Z",
     "start_time": "2023-01-17T13:48:05.888699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's visualize our topics in our feature space to see how divergent they were\n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim_models.prepare(my_lda, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the dominant topic in each sentence\n",
    "\n",
    "One of the practical application of topic modeling is to determine what topic a given document is about.\n",
    "\n",
    "To find that, we find the topic number that has the highest percentage contribution in that document.\n",
    "\n",
    "The `format_topics_sentences()` function below nicely aggregates this information in a presentable table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.889694Z",
     "start_time": "2023-01-17T13:48:05.889694Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=my_lda, corpus=corpus, texts=papers['abstract_processed_lemma']):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=my_lda, corpus=corpus, texts=papers['abstract'])\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "pd.set_option('max_colwidth', 400)\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the prevalence of a topic by each document\n",
    "\n",
    "The `.get_document_topics` method returns the probability of a topic belonging to any particular document. Investigate individually or apply against your original dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.890691Z",
     "start_time": "2023-01-17T13:48:05.890691Z"
    }
   },
   "outputs": [],
   "source": [
    "my_lda.get_document_topics(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextualizing topics\n",
    "\n",
    "Sometimes just the topic keywords may not be enough to make sense of what a topic is about. So, to help with understanding the topic, you can find the documents a given topic has contributed to the most and infer the topic by reading that document. Whew!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.891688Z",
     "start_time": "2023-01-17T13:48:05.891688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you think these topics are related to? Consider filtering on the df 'df_dominant_topic' to provide more context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Tagging\n",
    "\n",
    "![tagging](assets/tagging.png)\n",
    "\n",
    "\n",
    "Over the course of this lesson we've looked at a few ways to generate potential keywords for articles.\n",
    "\n",
    "1. Leveraging already labeled data\n",
    "2. Named Entity Extraction\n",
    "3. Topic Modeling\n",
    "\n",
    "\n",
    "Once this information is created - it can be used discriminantly to apply keywords to your documents to improve information retrieval. Since it's important for tags to be governed to maximize interprability and effectiveness the challenge switches from a generation exercise to a text classification exercise.\n",
    "\n",
    "Depending on the complexity of your model and the size of your tags - this could be an easy to challenging program\n",
    "\n",
    "### Tags\n",
    "\n",
    "Tags should be representative of the subject matter at hand. They begin at higher levels and can dig into more nuance as you gain the ability. For instance one might start simply with subjects\n",
    "\n",
    "- Biology\n",
    "- Chemistry\n",
    "- Physics\n",
    "- Mathematics\n",
    "\n",
    "\n",
    "As you can additional labeled data you can expand your tags to flush out any particular area. For instance:\n",
    "\n",
    "- Biology\n",
    "    - Molecular biology\n",
    "    - Cell Biology\n",
    "    - Genomics\n",
    "    - Proteomics \n",
    "    - Virology\n",
    "    - Ecology\n",
    "    - etc \n",
    "    \n",
    "    \n",
    "### Let's start by trying out Named Entity Recognition (or extraction)\n",
    "\n",
    "For this exercise we'll use a Seasonality of Transmission dataset from a larger COVID-19 repository on Kaggle\n",
    "\n",
    "https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.892686Z",
     "start_time": "2023-01-17T13:48:05.892686Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create the dataset\n",
    "\n",
    "COVID=pd.read_csv('./data/Seasonalityoftransmission.csv')\n",
    "COVID.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.893684Z",
     "start_time": "2023-01-17T13:48:05.893684Z"
    }
   },
   "outputs": [],
   "source": [
    "COVID.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.895678Z",
     "start_time": "2023-01-17T13:48:05.895678Z"
    }
   },
   "outputs": [],
   "source": [
    "COVID.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.897073Z",
     "start_time": "2023-01-17T13:48:05.897073Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notice the dataset has repeating values to allow it to categorize based on 'Study Type' and 'Factors'\n",
    "len(COVID['Study'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.899483Z",
     "start_time": "2023-01-17T13:48:05.899483Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's see how this works for a record in our dataframe\n",
    "COVID.Study[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.899990Z",
     "start_time": "2023-01-17T13:48:05.899990Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's see how this works for an individual record\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "doc=nlp(COVID.Study[0])\n",
    "\n",
    "print (doc)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.901447Z",
     "start_time": "2023-01-17T13:48:05.901447Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's see how this works for all the other records\n",
    "\n",
    "for i, study in enumerate(COVID.Study):\n",
    "    doc=nlp(study)\n",
    "    print(i, doc)\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print('-----Break-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interesting breakout of items but let's add some of our own words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.902917Z",
     "start_time": "2023-01-17T13:48:05.902917Z"
    }
   },
   "outputs": [],
   "source": [
    "# Start by creating a label\n",
    "LABEL = \"COVID\"\n",
    "\n",
    "# Add some training data associating the entries with beginning and ending character\n",
    "TRAIN_DATA = [\n",
    "    (\"Causal empirical estimates suggest COVID-19 transmission rates are highly seasonal\",\n",
    "        {\"entities\": [(35, 42, LABEL)]}),\n",
    "    (\"Meteorological factors correlate with transmission of 2019-nCoV: Proof of incidence of novel coronavirus pneumonia in Hubei Province, China\", \n",
    "     {\"entities\": [(54,62,LABEL)]}),\n",
    "    (\"Eco-epidemiological assessment of the COVID-19 epidemic in China, January-February 2020\",\n",
    "        {\"entities\": [(38, 45, LABEL)]}),\n",
    "    \n",
    "    (\"Correlation between weather and Covid-19 pandemic in Jakarta, Indonesia\",\n",
    "        {\"entities\": [(32, 39, LABEL)]}),\n",
    "    (\"Effects of temperature and humidity on the spread of COVID-19: A systematic review\",\n",
    "        {\"entities\": [(53, 60, LABEL)]} ),\n",
    "    (\"Effect of Temperature on the Transmission of COVID-19: A Machine Learning Case Study in Spain\", \n",
    "         {\"entities\": [(45, 52, LABEL)]}),\n",
    "    (\"COVID-19: Effects of weather conditions on the propagation of respiratory droplets\", \n",
    "         {\"entities\": [(0, 7, LABEL)]}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.905771Z",
     "start_time": "2023-01-17T13:48:05.905771Z"
    }
   },
   "outputs": [],
   "source": [
    "# It'll take too long to do that by hand - let's build a function that does this for us\n",
    "\n",
    "def build_training(df_series, word, LABEL):\n",
    "    \"\"\"Takes the word of interest, assigned to a LABEL against a dataframe series/column to return training data. \n",
    "    Note - this is only designed for one tag per entry\"\"\"\n",
    "    train_data=[]\n",
    "    \n",
    "    #Captures the ending position of the word within the string\n",
    "    y=len(word)-1\n",
    "    /\n",
    "    #Loops through each entry to find the word\n",
    "    for entry in set(df_series):\n",
    "        x= str(entry).lower().find(word.lower())\n",
    "        \n",
    "        if x == -1: # -1 means it wasn't found\n",
    "            continue # so we'll skip that one\n",
    "        else:\n",
    "            print (f'Word extracted: {entry[x:x+y+1]}')\n",
    "            train_data.append((entry,{\"entities\":[(x,x+y+1,LABEL)]}))\n",
    "    return train_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.906959Z",
     "start_time": "2023-01-17T13:48:05.906959Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create our training_data\n",
    "train_data=build_training(COVID.Study, word='Coronavirus', LABEL='DISEASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.907966Z",
     "start_time": "2023-01-17T13:48:05.907966Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Not much data is there?\n",
    "train_data[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.910382Z",
     "start_time": "2023-01-17T13:48:05.910382Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data[0][0][40:51]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model\n",
    "\n",
    "First, let’s understand the ideas involved before going to the code.\n",
    "\n",
    "1. To train an ner model, the model has to be looped over the example for sufficient number of iterations. If you train it for like just 5 or 6 iterations, it may not be effective.\n",
    "\n",
    "2. Before every iteration it’s a good practice to shuffle the examples randomly `throughrandom.shuffle()` function. This will ensure the model does not make generalizations based on the order of the examples.\n",
    "\n",
    "3. The training data is usually passed in batches.\n",
    "\n",
    "You can call the `minibatch()` function of spaCy over the training data that will return you data in batches . The minibatch function takes size parameter to denote the batch size. You can make use of the utility function compounding to generate an infinite series of compounding values.\n",
    "\n",
    "`compunding()` function takes three inputs which are start ( the first integer value) ,stop (the maximum value that can be generated) and finally compound. This value stored in compund is the compounding factor for the series.\n",
    "\n",
    "For each iteration , the model or ner is updated through the `nlp.update()` command. Parameters of `nlp.update()` are :\n",
    "\n",
    "- docs: This expects a batch of texts as input. You can pass each batch to the zip method, which will return you batches of text and annotations. `\n",
    "- vgolds: You can pass the annotations we got through zip method here\n",
    "\n",
    "- vdrop: This represents the dropout rate.\n",
    "\n",
    "- vlosses: A dictionary to hold the losses against each pipeline component. Create an empty dictionary and pass it here.\n",
    "\n",
    "At each word, the `update()` it makes a prediction. It then consults the annotations to check if the prediction is right. If it isn’t , it adjusts the weights so that the correct action will score higher next time.\n",
    "\n",
    "Finally, all of the training is done within the context of the nlp model with disabled pipeline, to prevent the other components from being involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.910382Z",
     "start_time": "2023-01-17T13:48:05.910382Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the spacy model (trained pipeline for English). For more details, visit https://spacy.io/models/en#en_core_web_sm\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "def adding_entity(train_data,LABEL):\n",
    "    \"\"\"Takes in the training data and associated entity label to attempt to learn a new entity\"\"\"\n",
    "    \n",
    "    random.seed(1)\n",
    "    \n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "        # otherwise, get the NER component, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # Add the new label to ner\n",
    "    ner.add_label(LABEL)\n",
    "\n",
    "    # Resumee training\n",
    "    optimizer = nlp.resume_training()\n",
    "    move_names = list(ner.move_names)\n",
    "\n",
    "    # List of pipes you want to train\n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "\n",
    "    # List of pipes which should remain unaffected in training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "    \n",
    "        # show warnings for misaligned entity spans once\n",
    "        warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "        # Training for 30 iterations     \n",
    "        for itn in range(30):\n",
    "            # shuffle examples before training\n",
    "            random.shuffle(train_data)\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(train_data, size=sizes)\n",
    "            # Dictionary to store losses\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                \n",
    "                example=[]\n",
    "                # Update the model with iterating each text\n",
    "                for i in range(len(texts)):\n",
    "                    doc = nlp.make_doc(texts[i])\n",
    "                    example.append(Example.from_dict(doc, annotations[i]))\n",
    "                \n",
    "                # Update the model\n",
    "                nlp.update(example, sgd=optimizer, drop=0.25, losses=losses)\n",
    "                print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.911820Z",
     "start_time": "2023-01-17T13:48:05.911820Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adding our training data to find the new word\n",
    "adding_entity(train_data, LABEL='DISEASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.913189Z",
     "start_time": "2023-01-17T13:48:05.913189Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "# nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "test_text = \"Eco-epidemiological assessment of the Coronavirus epidemic in China, January-February 2020\"\n",
    "#test_text = \"Correlation between weather and Covid-19 pandemic in Jakarta, Indonesia\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Entities in '%s'\" % test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)\n",
    "    \n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fantastic! A New Entity for Coronavirus!\n",
    "\n",
    "spaCy was able to look at how the word was being used in context, classify it and come out with a new way to understand and thus track this entity. However - with just a few examples, we may have needed to run a few times to return the new\n",
    "\n",
    "If spaCy can't recognize the word - in a case we'll show below with COVID-19  - then you'll need to add it to the dictionary. We try that in the below to show you what might happen. Sometimes this will even end up in a \"catastrophic failure\" where it lost the ability to find other Named Entities as well! Always something to be aware of when retraining a model and potentially a great use case for building a blank model against your target NER items (all your keywords)\n",
    "\n",
    "One solution would be adding COVID-19 to the lexicon/vocabulary of the model. This is a great situation to dig into [the spaCy documentation](https://spacy.io/usage/spacy-101).\n",
    "\n",
    "\n",
    "Some other food for thought. Often for spaCy to recognize a new entity it needs a few hundred observations. We used a randomseed here to make sure it found the entity with only two observations. However - the more the merrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.915577Z",
     "start_time": "2023-01-17T13:48:05.915577Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Updates our Training data (Adding old data on top of new data)\n",
    "train_data=build_training(COVID.Study, 'COVID-19', 'DISEASE')\n",
    "train_data = train_data + build_training(COVID.Study, 'Coronavirus', 'DISEASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.916582Z",
     "start_time": "2023-01-17T13:48:05.916582Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (f'No. of records in training data: {len(train_data)}')\n",
    "train_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.918008Z",
     "start_time": "2023-01-17T13:48:05.918008Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data[0][0][39:47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.920116Z",
     "start_time": "2023-01-17T13:48:05.919009Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build the model\n",
    "adding_entity(train_data, 'DISEASE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T13:48:05.921117Z",
     "start_time": "2023-01-17T13:48:05.921117Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "\n",
    "test_text = \"Eco-epidemiological assessment of the COVID-19 epidemic in China, January-February 2020\"\n",
    "# test_text = \"Eco-epidemiological assessment of the Coronavirus epidemic in China, January-February 2020\"\n",
    "# test_text = \"Correlation between weather and Covid-19 pandemic in Jakarta, Indonesia\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Entities in '%s'\" % test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)\n",
    "    \n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Classification method on your discovered topics\n",
    "\n",
    "### Step 1 - Define your tags\n",
    "- Avoid Overlapping\n",
    "- Don't mix classifications\n",
    "- Organize your tags around similar hierarchies\n",
    "\n",
    "\n",
    "### Step 2 - Data gathering\n",
    "\n",
    "Capture all the labeled data available for each tag. Consider what you have on hand, what you may want to build and potentially data you might need to build. Options range from crowdsourcing it with something like [AWS groundtruth](https://aws.amazon.com/sagemaker/groundtruth/) or algorithms like the [RAKE: Rapid Automatic Keyword Extractor](https://medium.com/datadriveninvestor/rake-rapid-automatic-keyword-extraction-algorithm-f4ec17b2886c)\n",
    "\n",
    "### Step 3 - Text classifier\n",
    "\n",
    "Determine the right text classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Tagging\n",
    "\n",
    "Now that you have a dominant topic tagged to all 1,000 articles from the NIPS papers - let's see if you can find a way to classify 500 new documents against the top topic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "\n",
    "In the world of NLP - context is king. In the above we learned to\n",
    "\n",
    "- Leverage POS and NER to provide a better understanding of words within the corpus\n",
    "- Transpose the DTM to give additonal context to words based on their usage\n",
    "- Build models to allow us to identify topics that exist within your corpus\n",
    "- Apply topics against your dataframe to continue your projects design\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "315px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
