# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Natural Language Processing - Continuing with expanded Context

> Unit 4: Flex

---

## Materials

| Topic | Description | Link |
| --- | --- | --- |
| Lesson Part 1| NLP - Word Embeddings & Latent Variables | [Here](./Context_to_Topics.ipynb) |
| Lesson Part 2| NLP - Topic Models | [Here](./Topic_Modeling.ipynb) |

---

## Installation Instructions:

Before this lesson students should install three packages

1. Gensim - `pip install gensim`
2. pyLDAvis - `pip install pyldavis`
3. spaCy
	-Step 1: `pip install spacy` 
	-Step 1: `python -m spacy download en`


---

## Learning Objectives

After this lesson, students will be able to:
- Remember the techniques from the previous lesson on word vectorization
- Investigate additional approaches to expand NLP context
- Demonstrate an understanding of clustering/topic approaches with NLP
- Understand approaches to persisting the insights from unsupervised approaches
- Discuss keyword generation and classification using machine learning

---

## Student Requirements

Before this lesson(s), students should already be able to:
- Understand covariance, and correlation.
- Write NumPy code to perform linear algebra matrix operations.
- Use Pandas and Seaborne to visualize data.


---

## Additional Resources


#### Papers covering maths in today's lesson in more detail
- [Original paper on Word2vec](https://arxiv.org/abs/1301.3781)
- [Topic Modeling high level maths](https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05)
- [Designing extraction for biomedical topics](https://arxiv.org/abs/2010.00074)
- [Evaluating an LDA topic model](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0)

#### Common Models concepts gaining prevalence in this space 

- [BERT - Bidirectional Encoder Representations from Transformers](https://www.geeksforgeeks.org/explanation-of-bert-model-nlp/)
- [Autoencoders](https://mc.ai/introduction-to-autoencoders/)
- [LSTM - Long Short Term Memory Models](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)

#### As these models continue to emerge it's helpful to review
- Sites such as the with over 1,400 models supporting inference across Sequence Classification, Text Generation, Question Answering, Token Classification with NER/POS, SUmmarization, NL inference, Conversational AI, Machine Translation, Text-to-speech and Commonsense Reasoning
    - [NLP model forge overview](https://medium.com/towards-artificial-intelligence/the-nlp-model-forge-a46faac7b5b0)
    - [Models](https://models.quantumstat.com/)
- Follow key individuals like [Phillip Vollet](https://www.linkedin.com/in/philipvollet/) to stay apraised of updates in the space
