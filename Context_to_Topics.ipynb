{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Continuing with Natural Language Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "- Remember the techniques from the previous lesson on word vectorization\n",
    "- Investigate additional approaches to expand NLP context\n",
    "- Demonstrate an understanding of word-embedding\n",
    "- Understand the purpose of latent variable models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please install the following libraries before starting this lesson.\n",
    "\n",
    "**spacy**  - `pip install spacy` followed by `python -m spacy download en_core_web_sm`\n",
    "\n",
    "**gensim** - `pip install gensim`\n",
    "\n",
    "**pyldavis** - `pip install pyldavis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:21.186203Z",
     "start_time": "2023-01-17T12:26:21.166499Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "from gensim import matutils, corpora\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import MmCorpus, dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "#import pyLDAvis.gensim_models\n",
    "\n",
    "pd.options.display.float_format = '{:,.8f}'.format\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications vs Mathematics and Design\n",
    "\n",
    "\n",
    "As you progress further into the world of Machine Learning and Natural Language Processing, it becomes more and more imperative to understand the underlying mathematics. For the purpose of today's lesson we are going to focus on overviews and applications. For those wishing to continue further please consider the below articles and references to help explain the underlying maths and principles behind the techniques & libraries discussed here\n",
    "\n",
    "\n",
    "- [Original paper on Word2vec](https://arxiv.org/abs/1301.3781)\n",
    "- [Topic Modeling high level maths](https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05)\n",
    "- [Designing extraction for biomedical topics](https://arxiv.org/abs/2010.00074)\n",
    "- [Evaluating an LDA topic model](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0)\n",
    "\n",
    "#### The field is rapidly growing with numerous techniques continually \n",
    "\n",
    "- [BERT - Bidirectional Encoder Representations from Transformers](https://www.geeksforgeeks.org/explanation-of-bert-model-nlp/)\n",
    "- [Autoencoders](https://mc.ai/introduction-to-autoencoders/)\n",
    "- [LSTM - Long Short Term Memory Models](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
    "\n",
    "#### As these models continue to emerge it's helpful to review\n",
    "- Sites with over 1,400 models supporting inference across Sequence Classification, Text Generation, Question Answering, Token Classification with NER/POS, SUmmarization, NL inference, Conversational AI, Machine Translation, Text-to-speech and Commonsense Reasoning\n",
    "    - [NLP model forge overview](https://medium.com/towards-artificial-intelligence/the-nlp-model-forge-a46faac7b5b0)\n",
    "    - [Models](https://models.quantumstat.com/)\n",
    "- Follow key individuals like [Phillip Vollet](https://www.linkedin.com/in/philipvollet/) to stay apraised of updates in the space\n",
    "\n",
    "All the above are pieces to a larger puzzle you are trying to solve with NLP. While we can overview the techniques today - every machine learning problem you encounter will require extensive design spanning from preprocessing strategies to how you extract and either apply or persist your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building on our understanding on NLP\n",
    "\n",
    "In the last lesson you learned the basics of mining unstructured data including preprocessing, building linguistic rules to uncover patterns and creating a basic classifier on review data. In addition you saw the build in power associated with NLP libraries with basics such as sentiment analysis, objectivity analysis and additional features such as spell check.\n",
    "\n",
    "The challenge in all NLP problems begins with the transformation of unstructured words into numbers. Those numbers must maintain some meaning to allow the analysis to continue. With a basic approach to vectorizing words you learned the power of word presence within a document using a bag-of-word method (countvectorizer) or capturing the importance of a word through its frequency (Term Frequency-Inverse Document Frequency). Both are helpful for simple tasks.\n",
    "\n",
    "The challenge becomes when we work on more complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's all about context\n",
    "\n",
    "**Context** is the hardest component of machine understanding of human language. While the world is innundated with chatbots, voice assistants and mining algorithms - the field of NLP is only about 30% of the way toward replicating the human intelligence of linguistic.\n",
    "\n",
    "\n",
    "And it's no surprise - it's hard! Language is imperfect, filled with personal contexts, idiomatic expressions and regional colloquialisms. It's the common things Machines can excel at and the uncommon... well we're getting better but not there yet. At the end of the day\n",
    "\n",
    "![context is king](assets/context-is-king.jpg)\n",
    "\n",
    "#### Consider the below.\n",
    "\n",
    "- Shut up. You are being rude.\n",
    "\n",
    "- OMG Shut up!\n",
    "\n",
    "**What do you think is the intended meaning of \"shut up\" in each sentence?**\n",
    "\n",
    "It can happen at the level of a word simply using a piece of punctiation\n",
    "\n",
    "- There\n",
    "\n",
    "- There!\n",
    "\n",
    "**How might these two hold different meaning?**\n",
    "\n",
    "\n",
    "As you can imagine - the way data is designed to capture this nuance is incredibly important if we wish to hold onto the context of what someone is trying to communicate. While the prevalence of frequency of words can be important - communication is often concerned about context. It's these nuances in words, both spoken and written, that hold completely seperate meaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we build on context?\n",
    "\n",
    "Before with our bag of words models we couldn't really address context - only prevalence of words. One way we can build on our understanding is through **Part of Speech Tagging**.\n",
    "\n",
    "Consider the word **bank** - How many meanings do you associate it with? \n",
    "\n",
    "\n",
    "#### **What if I said limit it to a noun**?\n",
    "\n",
    "\n",
    "#### **How about a verb**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech (POS) Tags\n",
    "\n",
    "POS tagging is the process of marking up a word in a corpus to a corresponding part of a speech tag, based on its context and definition. This task is not straightforward, as a particular word may have a different part of speech based on the context in which the word is used.\n",
    "\n",
    "\n",
    "There are different techniques for POS Tagging:\n",
    "1. **Lexical Based Methods** — Assigns the POS tag the most frequently occurring with a word in the training corpus.\n",
    "\n",
    "\n",
    "2. **Rule-Based Methods** — Assigns POS tags based on rules. For example, we can have a rule that says, words ending with “ed” or “ing” must be assigned to a verb. Rule-Based Techniques can be used along with Lexical Based approaches to allow POS Tagging of words that are not present in the training corpus but are there in the testing data.\n",
    "\n",
    "\n",
    "3. **Probabilistic Methods** — This method assigns the POS tags based on the probability of a particular tag sequence occurring. Conditional Random Fields (CRFs) and Hidden Markov Models (HMMs) are probabilistic approaches to assign a POS Tag.\n",
    "\n",
    "\n",
    "4. **Deep Learning Methods** — Recurrent Neural Networks can also be used for POS tagging.\n",
    "\n",
    "By understanding the word's POS - we can better understand the intended meaning. Furthermore by building \"parse trees\" we are able to build other contextual items like Named Entity Recognition Systems and Entity Linking capabilities.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center><img src='assets/spacy.png' alt='sPacy' width='300'></center>\n",
    "\n",
    "\n",
    "[Spacy](https://spacy.io/) is an open-source software library for advanced natural language processing, written in the programming languages Python. Unlike NLTK, which is widely used for teaching and research, spaCy focuses on providing software for production usage. Key abilities include:\n",
    "\n",
    "- Non-destructive tokenization\n",
    "- Named entity recognition\n",
    "- Support for 59+ languages\n",
    "- 46 statistical models for 16 languages\n",
    "- Pretrained word vectors\n",
    "- State-of-the-art speed\n",
    "- Easy deep learning integration\n",
    "- Part-of-speech tagging\n",
    "- Labelled dependency parsing\n",
    "- Syntax-driven sentence segmentation\n",
    "- Built in visualizers for syntax and NER\n",
    "- Convenient string-to-hash mapping\n",
    "- Export to numpy data arrays\n",
    "- Efficient binary serialization\n",
    "- Easy model packaging and deployment\n",
    "- Robust, rigorously evaluated accuracy\n",
    "\n",
    "\n",
    "In the below we will review it for POS tagging, Named Enttty recognition and named entity linking. However, those interested in NLP should review it's offerings at a greater depth for its range of applications. You can  see a list of [Universal POS tags here](https://universaldependencies.org/docs/u/pos/).\n",
    "\n",
    "\n",
    "### POS Tagging\n",
    "In the below we take a boiler plate example of spacey to show how it is able to parse sentences into their component parts. This example is limited to displaying the word, its associated POS and any additional details contained in the Tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:21.660119Z",
     "start_time": "2023-01-17T12:26:21.187200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading in the Spacy Model\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Pass the model a sentence to review\n",
    "doc = nlp(\"Apple is looking at buying a U.K. startup for $1 billion\")\n",
    "\n",
    "#Show the ability to parse out each word. \n",
    "for token in doc:\n",
    "    print(f'\"{token.text}\" is a \"{token.pos_}\" tagged as a \"{token.tag_}\"')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy will also natively map the relationships between words to show associations built on traditional grammer rules and conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:21.675796Z",
     "start_time": "2023-01-17T12:26:21.662198Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can take this further by using displacy to render how words are tied together\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:21.706780Z",
     "start_time": "2023-01-17T12:26:21.677792Z"
    }
   },
   "outputs": [],
   "source": [
    "# There are ranges of other features including the ability to capture, dissect and retain structure for noun chunks\n",
    "\n",
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(f'{chunk.text}  | {chunk.root.text}   | {chunk.root.dep_}   |{chunk.root.head.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition\n",
    "\n",
    "Named Entity Recognition (NER) - is an information extraction technique that automatically identifies named entities in a text and classifies them into predefined categories. Entities can be names of people, organizations, locations, times, quantities, monetary values, percentages, and more.\n",
    "\n",
    "With named entity recognition, you can obtain key information to understand what a text is about, making it a great starting point for all kinds of text analysis and data organization.\n",
    "\n",
    "### NER falls into four primary approaches:\n",
    "\n",
    "1. **Lexicon Approach** - Relying on a knowledge base containing all the words or terms related to a particular topic, grouped into categories. While this can be a strong approach, it is solely dependent on the data/dictionaries you have on hand. The obvious downside is it does not work for terms not in the lexicon\n",
    "\n",
    "\n",
    "2. **Rule-based systems** - Employ a series of grammatical rules hand-crafted by computational linguists. Rules work well to extract entities like street names, phone numbers, social security numbers, or any other type of data that follows specific patterns.\n",
    "\n",
    "    With rule-based systems, you can get results of high precision but low recall. This means that, while most of the predictions for predefined categories are true positives (e.g, the majority of the words that a model tags as “company name” are actually companies), the ability of a model to identify all relevant instances a company is mentioned is low. Defining rules and patterns takes time and they can’t be adapted to new domains; they only work well for the purpose they’ve been created, and it’s hard to modify them.\n",
    "    \n",
    "    \n",
    "3. **Machine Learning Based Systems** - learn to recognize entities in text based on previous examples. To build an entity extractor, you must begin with a large volume of labeled training data completed with both positives and negatives so that it can _learn_ what an entity is. \n",
    "\n",
    "\n",
    "4. **Hybrid Approach** - Traditionally a hybrid approach builds on a rules-based approach by adding a machine learning approach. The combination will allow you to extract entities with a high level of precision.\n",
    "\n",
    "\n",
    "### Real-life use cases\n",
    "\n",
    "1. **Article Tagging** - designing an internal search algorithm for an online publisher that has millions of articles. If for every search query the algorithm ends up searching all the words in millions of articles, the process will take a lot of time. Instead, if Named Entity Recognition can be run once on all the articles and the relevant entities (tags) associated with each of those articles are stored separately, this could speed up the search process considerably. With this approach, a search term will be matched with only the small list of entities discussed in each article leading to faster search execution.\n",
    "\n",
    "2. **Recommendation Systems** - An expanding use case is adding additional meta-data to recommendation systems. NER can extract entities from news articles or summary documents then recommending similar entities mentioned based on a users choices\n",
    "\n",
    "3. **Ticket Categorization** - Automate repetitive tasks by leveraging entity extraction to pull relevant pieces of data from your incoming tickets, like company names, product names, or series numbers, making it easier to route tickets to the most suitable agent or team for handling that issue.\n",
    "\n",
    "4. **Customer Feedback** - Enhance survey responses by using NER to extract locations and products to filter/route feedback appropriately\n",
    "\n",
    "5. **Resume Review** - Quickly parse out contact information, certifications and relevant experience\n",
    "\n",
    "\n",
    "#### In the below we'll leverage sPacy to explore NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:22.323011Z",
     "start_time": "2023-01-17T12:26:21.708285Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"Apple is looking at buying a U.K. startup for $1 billion. I want a red apple.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try it yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:36.713789Z",
     "start_time": "2023-01-17T12:26:22.324522Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run the following and input your own sentence.\n",
    "\n",
    "text = input()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing a system with POS and NER\n",
    "\n",
    "Now that you know how to capture a POS or a NER - what's next? Being able to extract this context is fundamental in the preprocessing of larger models. By including them you are able to add additional context that will allow a followon model to be more generative or discriminatory depending on the purpose\n",
    "\n",
    "\n",
    "### Where could these help with the model's we've discussed in class so far?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Variable models\n",
    "\n",
    "With the focus on **context** the goal of today's lesson to continue with methods to refine the extraction of structure, organization and/or context in the text. Some of these methods come from having larger dictionarys, more data or expanded methods to build toward the underpinning of language and its underlying meaning.\n",
    "\n",
    "Latent variables are variables that are not directly observed but are rather inferred (through a mathematical model) from other variables that are observed (directly measured).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike our previous methods - **Latent Variable Models** are different in that they try to understand language based on **how** the words are used. We assume the data we are observing has some hidden, underlying structure that we can’t see, and which we’d like to learn.\n",
    "\n",
    "These hidden, underlying structures are the latent (i.e. hidden) variables we want our model to understand.\n",
    "\n",
    "\n",
    "In the previous lesson we learned we could equate 'bad' and 'badly' as they share the same root. Today we'll determine they are related because they are often used in the same way or near similar words.\n",
    "\n",
    "\n",
    "|Traditional NLP Models|Latent Variable Models|\n",
    "|------|------|\n",
    "|Focused on theoretical understanding of language  |Focused on how the language is actually used in practice    |\n",
    "|Tries to learn the rules of a particular language |Infers meaning from how words are used together    |\n",
    "|Preprogrammed set of rules |Uses unsupervised learning to discover patterns or structure   |\n",
    "\n",
    "\n",
    "There arises a problem that we'll soon discover needs to be addressed through all the latent models we'll explore below - Size of the matrix. By trying to add in context we run the risk of creating a very large, noisy and sparse matrix. Thus enters the need for dimensionality reduction.\n",
    "\n",
    "There are many techniques to perform dimensionality reduction automatically and most follow a very similar approach\n",
    "\n",
    "1. Identify correlated columns\n",
    "2. Replace them with a new column that encapsulates the others\n",
    "\n",
    "![dimensionalityreduction](./assets/dimreduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings - Flipping our understanding of the Document Term Matrix\n",
    "\n",
    "The core concept of word embeddings is that every word used in a language can be represented by a set of real numbers (a vector). If we take our usual document-word matrix and take its transpose, instead of talking about words as being features of a document, we can talk about documents as being features of a specific word.\n",
    "\n",
    "#### Word embeddings open a variety of possible applications:\n",
    "\n",
    "- Automatic summarization\n",
    "- Machine translation \n",
    "- Named entity resolution\n",
    "- Sentiment analysis\n",
    "- Information retrieval\n",
    "- Speech recognition\n",
    "- Question answering\n",
    "- Music/Video Recommendation\n",
    "\n",
    "\n",
    "All of these are powered by finding context and determining how similar an item is to another. As the goal is to transform words into vectors - one often used method is **cosine similarity** - a measurement of similarity between two non-zero vectors of an inner product space.\n",
    "\n",
    "\n",
    "![CosineSimilarity](assets/cosine.png)\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"https://dataaspirant.com/five-most-popular-similarity-measures-implementation-in-python/\" title=\"Cosine Similarity\">Source</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec - Exploring word embeddings\n",
    "\n",
    "\n",
    "<center><img src='assets/word2vecmodel.png' alt='Word2vec' width='750'></center>\n",
    "\n",
    "Word2Vec is one of the common libraries used for discovering and leveraging word embeddings in python. A word is defined by the company it keeps. That’s the premise behind Word2Vec as a method of converting words to numbers and representing them in a multi-dimensional space. Words frequently found close together in a collection of documents (corpus) will also appear close together in this space. They are said to be related contextually and allow us to capture the much needed context\n",
    "\n",
    "Word2vec is not a single algorithm but a combination of two techniques – \n",
    "\n",
    "   - **Continuous bag-of-words (CBOW)** — The order of context words does not influence prediction (bag-of-words assumption). In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words.\n",
    "   - **Continuous skip-gram** weighs nearby context words more heavily than more distant context words. While order still is not captured each of the context vectors are weighed and compared independently vs CBOW which weighs against the average context\n",
    "    \n",
    "Both of these are shallow neural networks which map one or more words to a target variable which is also one or more words. Both of these techniques learn weights which act as word vector representations. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence\n",
    "\n",
    "#### Example\n",
    "The most famous example of the context learned through word2vec is from the example below. Here we can see a few common word vectors based on a set of pretrained data\n",
    "\n",
    "<center><img src='assets/word2vec1.png' alt='Word2vec' width='600'></center>\n",
    "\n",
    "\n",
    "Through turning these words to vectors the following equation was possible.\n",
    "\n",
    "### (king – man) + woman = ?\n",
    "\n",
    "![Word2vec](assets/word2vec2.png)\n",
    "\n",
    "You can reproduce this result by utilizing the below code and this [file](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit). As this is based on a neural network - it needs quite a bit of data to understand nuance/context. Warning: This file is 3.4 gb unzipped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:36.729349Z",
     "start_time": "2023-01-17T12:26:36.713789Z"
    }
   },
   "outputs": [],
   "source": [
    "#Example - Do not run\n",
    "\n",
    "#from gensim.models import KeyedVectors\n",
    "\n",
    "# load the google word2vec model\n",
    "#filename = 'GoogleNews-vectors-negative300.bin'\n",
    "#model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "\n",
    "# calculate: (king - man) + woman = ?\n",
    "#result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trialing Word2Vec\n",
    "\n",
    "Lets see what we can do with less data. In the below we'll be doing an example with data from the longest living cartoon on the planet - [The Simpsons](https://www.kaggle.com/pierremegret/dialogue-lines-of-the-simpsons?select=simpsons_dataset.csv)\n",
    "\n",
    "We'll build the model in three steps:\n",
    "\n",
    "1. Build our model with parameters to allow it to parse through our data.\n",
    "2. Build the vocabulary from a sequence of sentences\n",
    "3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:36.804682Z",
     "start_time": "2023-01-17T12:26:36.731304Z"
    }
   },
   "outputs": [],
   "source": [
    "Simpsons=pd.read_csv('./data/simpsons.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set Parameters:\n",
    "\n",
    "- **min_count** = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "- **window** = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n",
    "- **size** = int - Dimensionality of the feature vectors. - (50, 300)\n",
    "- **sampl**e = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n",
    "- **alpha** = float - The initial learning rate - (0.01, 0.05)\n",
    "- **min_alpha** = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
    "- **negative** = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "- **workers** = int - Use these many worker threads to train the model (=faster training with multicore machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:36.820047Z",
     "start_time": "2023-01-17T12:26:36.806510Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Input\n",
    "Starting from the beginning, gensim’s word2vec expects a sequence of sentences as its input. Each sentence a list of words (utf8 strings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:37.045116Z",
     "start_time": "2023-01-17T12:26:36.821623Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = Simpsons.sentences.map(lambda sentences: sentences.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:37.071829Z",
     "start_time": "2023-01-17T12:26:37.047827Z"
    }
   },
   "outputs": [],
   "source": [
    "#Visualizing the split\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Building the Vocabulary \n",
    "Word2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:37.269054Z",
     "start_time": "2023-01-17T12:26:37.072979Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.build_vocab(sentences, progress_per=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training of the model:\n",
    "Parameters of the training:\n",
    "- total_examples = int - Count of sentences;\n",
    "- epochs = int - Number of iterations (epochs) over the corpus - [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:51.428265Z",
     "start_time": "2023-01-17T12:26:37.270078Z"
    }
   },
   "outputs": [],
   "source": [
    "# Will take few seconds for the model to train\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use our knowledge of the world's longest running cartoon to play around with some similarities. What is most similar to homer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:51.443860Z",
     "start_time": "2023-01-17T12:26:51.430388Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=[\"homer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is most similar to Principal Skinner??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:51.474633Z",
     "start_time": "2023-01-17T12:26:51.448029Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=[\"skinner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about looking at similarities between words within our corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:51.495868Z",
     "start_time": "2023-01-17T12:26:51.477474Z"
    }
   },
   "outputs": [],
   "source": [
    "model.wv.similarity(\"moe\", 'tavern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:51.511528Z",
     "start_time": "2023-01-17T12:26:51.497646Z"
    }
   },
   "outputs": [],
   "source": [
    "model.wv.similarity('maggie', 'baby')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Who is the odd one out in the below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:51.526655Z",
     "start_time": "2023-01-17T12:26:51.513678Z"
    }
   },
   "outputs": [],
   "source": [
    "model.wv.doesnt_match([\"nelson\", \"bart\", \"milhouse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now back to the (king – man) + woman = ? example. Which word is to \"woman\" as \"Bart\" is to \"man\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-17T12:26:51.541921Z",
     "start_time": "2023-01-17T12:26:51.528693Z"
    }
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=[\"girl\", \"bart\"], negative=[\"boy\"], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways:\n",
    "    \n",
    "1. Word embeddings allow us to build on our prior tokenization methods by adding \"context\" to the analysis\n",
    "2. Word2Vec allows us to flip our DTM transposing the focus from the document to the term\n",
    "3. Word2Vec has a built in dimensionality reduction model bringing similar contexts together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - [GloVe](https://nlp.stanford.edu/projects/glove/) is another method which adds a global context to Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "345.994px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
